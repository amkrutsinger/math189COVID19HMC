{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4: Automatic Differentiation Software + RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final homework, we will look at neural networks. The point of this homework is not to teach you everything about neural networks, but to make them a little less scary and give you the option of using one in your research! You will get a walkthrough of pytorch, a popular automatic differentiation package, modify a simple neural network, and run an RNN used in research on genetic data.\n",
    "\n",
    "### Automatic Differentiation\n",
    "Automatic differentiation is an incredibly powerful tool that allows you to perform gradient descent without keeping track of and calculating gradients yourself! If you need a refresher, (or introduction) on gradient descent, there are many resources online, and [this one](https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e) isn't bad. Come back when you are done to see the magic of gradient optimization in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, you need to download the pytorch (<3) package, as is described [here.](https://pytorch.org/) Having installed this successfully, follow along in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusingly, you use pytorch by importing torch, and we use these import torch.x as x\n",
    "# because that is what you will often see in their libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to do is a toy linear regression example. This time, we aren't using a package to do it all for us. Instead, we are going to find the line of best fit by minimizing the mean squared error between the points and the prediction of the line. To minimize the error, we are going to follow the gradient of our error, or \"loss\" function (actually go in the opposite direction of the gradient). \n",
    "\n",
    "\n",
    "However, instead of using a formula for the gradient, we are going to just do the operations and let pytorch calculate the gradient for us. Take your time reading this code, things will build on these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's initialize our dataset, which will have slope 1, \n",
    "# intercept 0, and gaussian noise added to each point. Let's see what\n",
    "# it looks like:\n",
    "x = torch.arange(10, dtype=torch.float32)\n",
    "# torch.randn(10) returns a vector of length 10 with entries sampled from a normal\n",
    "# distribution\n",
    "y = x + torch.randn(10)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Fake Y vs Fake X\")\n",
    "plt.xlabel(\"Fake X\")\n",
    "plt.ylabel(\"Fake Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a (rough) line! You may notice that some of those functions are also numpy functions! Pytorch has similar, but not identical, syntax to numpy, and much of the intuition you built up using numpy will apply here.\n",
    "\n",
    "Now when you are doing numerical optimization, you have to initialize your hypotheses somehow (so you have something to optimize), so lets do that now. Our \"hypothesis\" is that the data x and labels y have the relationship y = m * x + b for some m and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires grad tells pytorch it has to be ready to calculate the gradient for\n",
    "# these variables\n",
    "m = torch.tensor([-1], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a guess, let's plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "# We have to tell pytorch to not track this operation\n",
    "with torch.no_grad():\n",
    "    plt.plot(x.numpy(), (x*m + b).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that probably wasn't a good guess (if it was, reinitialize the variables). Let's tune our parameters to make a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is the hyperparameter that we set to make sure our steps are \"local\"\n",
    "# because gradient only describes local behavior\n",
    "learning_rate = 2e-3\n",
    "\n",
    "# Let's keep track of the slopes and intercepts so we can visualize what we have done\n",
    "slopes = []\n",
    "intercepts = []\n",
    "losses = []\n",
    "\n",
    "# This is a training loop, something you will find universally in gradient descent. We\n",
    "# need to take many little steps in the direction of the gradient\n",
    "for i in range(100):\n",
    "    \n",
    "    # Notice vectorized operations\n",
    "    prediction = b + m * x\n",
    "    \n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    loss = ((y - prediction) ** 2).mean()\n",
    "    \n",
    "    # This is again just for plotting our learning\n",
    "    if i % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            slopes.append(m.clone().detach().numpy())\n",
    "            intercepts.append(b.clone().detach().numpy())\n",
    "    \n",
    "    # Tell pytorch that these variables are important\n",
    "    \n",
    "    m.retain_grad()\n",
    "    b.retain_grad()\n",
    "    # After calculating loss, we can tell pytorch to calculate the gradient. \n",
    "    # This .backward() updates m.grad and b.grad to hold the gradients of the operations\n",
    "    # we just did (specifically prediction and MSE calculation)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        # Do the gradient descent step, notice MINIMIZING loss, so go in opposite\n",
    "        # direction of gradient. \n",
    "        m -= learning_rate * m.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Reset gradients to go around the loop again\n",
    "    m.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out all of the lines that we tried! Check out how the slope and intercept changed over time! It worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "for m, b in zip(slopes, intercepts):\n",
    "    print(f\"Slope: {m}, Intercept: {b}\")\n",
    "    plt.plot(x.numpy(), (b+x.clone().numpy()*m))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Time!\n",
    "Now we are going to create a neural network using this code. This is a big jump in complexity, but defining neural networks in pytorch is very formulaic, so if you take the time to understand this one it will be helpful for every neural network you see forever and ever. There are maybe more blog posts that describe how neural networks work than there are that describe gradient descent. \n",
    "\n",
    "[This book](http://neuralnetworksanddeeplearning.com/index.html) is very in depth and really guides you through math. If you want to go that deep, [this blogpost](https://victorzhou.com/blog/intro-to-neural-networks/) gives a shorter description and some intuition, and I highly recommend you at least check that out.\n",
    "\n",
    "The big idea behind a neural network is that a neural network can be trained to approximate a function with example input and output data. A neural network is composed of a series of trainable parameters, all of which are either \"weights\" which are entries in a matrix that are analogous to the slope in the linear regression example or \"biases\" that are analogous to the intercept. That means \"running\" a (traditional feedforward) neural network is equivalent to doing a series of matrix multiplications and vector additions, calculating the gradient of the error with respect to each of these operations using .backward(), and taking a step in the appropriate direction.\n",
    "\n",
    "The fact that automatic differetiation packages exist is a result of the [backpropogation](http://neuralnetworksanddeeplearning.com/chap2.html) algorithm, which is important to be aware of, but not necessary to fully understand in this excercise.\n",
    "\n",
    "Let's see this in practice! We will be building a neural network that can calculate the parity (even or odd sum) of a list of a fixed length. This is a relatively easy task, which is good because it trains fast, but it is complex enough that we can see how neural networks might be useful for even more complex operations.\n",
    "\n",
    "__Note:__ if you have never seen the softmax function before, you can read [this](https://towardsdatascience.com/softmax-function-simplified-714068bf8156) blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_perms(series):\n",
    "    return {p for p in itertools.permutations(series)}\n",
    "\n",
    "def construct_dataset(vec_len):\n",
    "    example_list = [0] * vec_len\n",
    "    data = [[0] * vec_len] \n",
    "    labels = [[1, 0]]\n",
    "    for i in range(vec_len):\n",
    "        example_list[i] = 1\n",
    "        permutations = list(unique_perms(example_list))\n",
    "        data.extend(permutations)\n",
    "        labels.extend([[i % 2, (i + 1) % 2]] * len(permutations))\n",
    "        \n",
    "        \n",
    "    return torch.FloatTensor(data), torch.FloatTensor(labels)\n",
    "\n",
    "# If you want to see what the data and labels looks like, try:\n",
    "# print(construct_dataset(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the usual setup for a pytorch neural network. You make a class that inherits\n",
    "# nn.Module, initialize parameters, and define a forward (a.k.a. predict) method.\n",
    "\n",
    "class MyFirstNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        #This line is required, but not important\n",
    "        super(MyFirstNetwork, self).__init__()\n",
    "        \n",
    "        # In this network we define two layers, a hidden layer and an output layer\n",
    "        # Remember, these layers are just matrices of dimension arg1 X arg2 with\n",
    "        # a vector of biases of dimension arg2 X 1. Pytorch combines that all into\n",
    "        # a single layer, making things super easy\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # You can pass data through a layer by calling that layer like a function!\n",
    "        # Check out below:\n",
    "        layer1_output = self.layer1(x)\n",
    "        \n",
    "        # One important part of many neural networks is adding a non-linearity\n",
    "        # function. This increases the complexity of functions neural networks\n",
    "        # can approximate. We are using the relu function in this example\n",
    "        # (https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). We want\n",
    "        # to use a non-linearity like relu after every layer (except for on the output\n",
    "        # then we want to use softmax or something like that).\n",
    "        h = F.relu(layer1_output)\n",
    "        \n",
    "        # Now we run the output of the previous layer through the output layer. We \n",
    "        # ultimately want to output whether there is even (1,0) or odd (0,1) parity.\n",
    "        # So we treat this output as representing unnormalized probabilities, or logits,\n",
    "        # and use the softmax function so we get a vector that sums to one. We then\n",
    "        # want to train the Network to output either (1, 0) or (0, 1), depending on\n",
    "        # the input\n",
    "        logits = self.layer2(h)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "# Another training loop! This time we allow the loss calculation to be any function\n",
    "# criterion, and we use pytorch's optimizer instead of doing the gradient descent by\n",
    "# hand. Finally, epoch is the word we use for an iteration through all of the data.\n",
    "# We are training on all of our data each step in this example, but you can also\n",
    "# train on mini batches of your data, e.g. 32 examples at a time, if your dataset\n",
    "# is big enough\n",
    "def train(network, data_matrix, labels, criterion, optimizer, num_epochs = 1000):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zeroing the gradient like before\n",
    "        optimizer.zero_grad()\n",
    "        # Getting the prediction from our model on all of the data\n",
    "        output = network(data_matrix)\n",
    "        # Calculating loss\n",
    "        loss = criterion(output, labels)\n",
    "        losses.append(loss)\n",
    "        # Calculate the gradients at each step\n",
    "        loss.backward()\n",
    "        # Take a step in the appropriate direction\n",
    "        optimizer.step()\n",
    "        \n",
    "    return losses\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the pieces, we can put them together and train a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_len = 8\n",
    "data, labels = construct_dataset(vector_len)\n",
    "\n",
    "# Let's initialize our network\n",
    "# our inputs will be length 8\n",
    "input_size = vector_len\n",
    "# A layer size that that feels right for our input size\n",
    "hidden_size = 16\n",
    "# our output will be a categorical distribution, (prob_even, prob_odd)\n",
    "output_size = 2\n",
    "# a hyperparameter (that you will change!)\n",
    "learning_rate = 0.03\n",
    "\n",
    "mynet = MyFirstNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# pytorch provides an mse calculator for us\n",
    "criterion = nn.MSELoss()\n",
    "# The optimizer we are using is called Adam, and it is like fancy gradient descent.\n",
    "# Worth checking out, but for right now we just need to know that it converges\n",
    "# faster on many problems. We pass the parameters, i.e. layers of our network to\n",
    "# the optimizer so that it can track them and update them, and we give a learning rate\n",
    "optimizer = optim.Adam(mynet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what our untrained network predicts!\n",
    "print(mynet(torch.FloatTensor([0,0,0,0,0,0,0,1]).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very sure either way, we can do much better than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put it all together and train the network!\n",
    "losses = train(mynet, data, labels, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)\n",
    "plt.title(\"MSE Per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what the network predicts when we run it on a datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet(torch.FloatTensor([1,0,0,0,0,0,0,0]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network to build off of, you are going to be tweaking different parts of the code so that you get more comfortable working with pytorch. You are going to get some practice plotting here. With each tweak, plot the original loss curve and the new loss curve after the tweak. \n",
    "1. Change the learning rate. Try differing by orders of 3x in both directions, and plot your results for 5 different learning rates. Plot 1 graph with all 5 loss curves.\n",
    "2. Change the size of the hidden layer. Try a much lower value and see if the network can still learn well. What is the smallest that layer can be and still learn the function? Plot the results of five different attempts, again on the same graph.\n",
    "3. Add another layer in myfirstnetwork to create mysecondnetwork. See if you can use 2 smaller hidden layers instead of one bigger layer. Don't forget the non-linearity!\n",
    "4. (Optional) Split the training data into training and testing data. See if you can get the network to generalize to data it hasn't seen before. __Note__ You should always do this when you are training a real network, because overfitting is a very common problem. (i.e. your network fails to generalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start coding here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Time!\n",
    "\n",
    "RNNs are inspired by HMMs and are used to process sequential data. If you need a refresher on RNNs, check out [this blogpost.](https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7) We will be implementing a vanilla recurrent neural network because it is conceptually the simplest, but it is very rare you would see one in the wild. Long-Short Term Memory networks (LSTMs) or GRUs are much more common in the real world, but are just variations on the key ideas you will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Side note:__ you can use tqdm for easy progress bars, pretty cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to construct a slightly different dataset to train the RNN. You may recall that an RNN outputs something at each timestep, and so to give it a stronger signal we will label each element with the parity so far in the sequence, as we want the RNN to be able to distinguish between even and odd summed sequences of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_perms(series):\n",
    "    return {p for p in itertools.permutations(series)}\n",
    "\n",
    "def construct_rnn_dataset(vec_len):\n",
    "    example_list = [0] * vec_len\n",
    "    data = [[0] * vec_len] \n",
    "    labels = [[0] * vec_len]\n",
    "    for i in range(vec_len):\n",
    "        example_list[i] = 1\n",
    "        permutations = list(unique_perms(example_list))\n",
    "        data.extend(permutations)\n",
    "        for perm in permutations:\n",
    "            labels.append([sum(perm[:i+1]) % 2 for i in range(len(perm))])\n",
    "        \n",
    "        \n",
    "    return torch.FloatTensor(data), torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fill out the forward function for this RNN. You may want to use MyFirstNetwork as inspiration. ```forward``` gives the output and hidden state after one timestep of the network. ```forward_predict``` gives you an idea of how forward would be used to run your RNN on an entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timestep:\n",
    "<pre>\n",
    "prev_hidden                              ----------> |i2o| ---------------> output\n",
    "    +        -----> |input2hidden1| -----|\n",
    "  input                                  ----------> |input2hidden2|------> hidden\n",
    "</pre>\n",
    "Don't forget to put nonlinearities after each layer (to make the network more expressive) and a softmax on the output (because we are classifying each sequence as either even or odd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        # This should look familiar!\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input2hidden1 = nn.Linear(input_size + hidden_size, input_size + hidden_size)\n",
    "        self.input2hidden2 = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # We will start you off with the torch.cat function\n",
    "        combined = torch.cat((input, hidden), -1)\n",
    "        # How do we get from combined to our new output and hidden?\n",
    "        return output, hidden\n",
    "\n",
    "    def forward_predict(self, sequence):\n",
    "        h = self.init_hidden(1)\n",
    "        for e in sequence:\n",
    "            o, h = self.forward(e.reshape(1,1), h)\n",
    "        return o\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def train(network, data_matrix, labels, criterion, optimizer, num_epochs = 200):\n",
    "    losses = []\n",
    "    # This should look familiar too!\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Zero out the gradient and initialize the loss...\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0.)\n",
    "        hidden = network.init_hidden(data_matrix.shape[0])\n",
    "        \n",
    "\n",
    "        # Getting the prediction from our model on all of the data\n",
    "        # at each timestep, and calculating the error in parity prediction\n",
    "        for i in range(data_matrix.shape[1]):\n",
    "            output, hidden = network(data_matrix[:,i].unsqueeze(1), hidden)\n",
    "            loss += criterion(output, labels[:,i]) / data_matrix.shape[1]\n",
    "        \n",
    "        losses.append(loss)\n",
    "        # Calculate the gradients at each step\n",
    "        loss.backward()\n",
    "        # Take a step in the appropriate direction\n",
    "        optimizer.step()\n",
    "        \n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's construct our dataset and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = construct_rnn_dataset(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time when we train, we will use CrossEntropyLoss instead of MSE loss. Cross entropy loss is more appropriate for training a classifier, as it treats output as probabilities so you can get a stronger gradient for a 0-1 flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mynet = RNN(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(mynet.parameters(), lr=.03)\n",
    "losses = train(mynet, data, labels, criterion, optimizer)\n",
    "\n",
    "plt.plot(np.arange(len(losses)), losses)\n",
    "\n",
    "    \n",
    "plt.title(\"Cross Entropy Loss Per Epoch with RNN\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy (bits)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained the network, if all is good we should be able to classify\n",
    "# a sequence of many lengths using forward predict. Try it out below and see for yourself!\n",
    "mynet.forward_predict(torch.FloatTensor([1,0,0,1,0,1,1,0,0,0,0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
